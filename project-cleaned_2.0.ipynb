{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amark\\anaconda3\\envs\\python3.5\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from utils import LRN2D\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createmodel():\n",
    "\n",
    "    myInput = Input(shape=(96,96,3))\n",
    "\n",
    "    x = ZeroPadding2D(padding=(3, 3), input_shape=(96,96,3))(myInput)\n",
    "    x = Conv2D(64, (7, 7), strides=(2, 2), name='conv1')(x)\n",
    "    x = BatchNormalization(axis=3, epsilon=0.00001, name='bn1')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "    x = MaxPooling2D(pool_size=3, strides=2)(x)\n",
    "    x = Lambda(LRN2D, name='lrn_1')(x)\n",
    "    x = Conv2D(64, (1, 1), name='conv2')(x)\n",
    "    x = BatchNormalization(axis=3, epsilon=0.00001, name='bn2')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "    x = Conv2D(192, (3, 3), name='conv3')(x)\n",
    "    x = BatchNormalization(axis=3, epsilon=0.00001, name='bn3')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Lambda(LRN2D, name='lrn_2')(x)\n",
    "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "    x = MaxPooling2D(pool_size=3, strides=2)(x)\n",
    "\n",
    "    # Inception3a\n",
    "    inception_3a_3x3 = Conv2D(96, (1, 1), name='inception_3a_3x3_conv1')(x)\n",
    "    inception_3a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_3x3_bn1')(inception_3a_3x3)\n",
    "    inception_3a_3x3 = Activation('relu')(inception_3a_3x3)\n",
    "    inception_3a_3x3 = ZeroPadding2D(padding=(1, 1))(inception_3a_3x3)\n",
    "    inception_3a_3x3 = Conv2D(128, (3, 3), name='inception_3a_3x3_conv2')(inception_3a_3x3)\n",
    "    inception_3a_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_3x3_bn2')(inception_3a_3x3)\n",
    "    inception_3a_3x3 = Activation('relu')(inception_3a_3x3)\n",
    "\n",
    "    inception_3a_5x5 = Conv2D(16, (1, 1), name='inception_3a_5x5_conv1')(x)\n",
    "    inception_3a_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_5x5_bn1')(inception_3a_5x5)\n",
    "    inception_3a_5x5 = Activation('relu')(inception_3a_5x5)\n",
    "    inception_3a_5x5 = ZeroPadding2D(padding=(2, 2))(inception_3a_5x5)\n",
    "    inception_3a_5x5 = Conv2D(32, (5, 5), name='inception_3a_5x5_conv2')(inception_3a_5x5)\n",
    "    inception_3a_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_5x5_bn2')(inception_3a_5x5)\n",
    "    inception_3a_5x5 = Activation('relu')(inception_3a_5x5)\n",
    "\n",
    "    inception_3a_pool = MaxPooling2D(pool_size=3, strides=2)(x)\n",
    "    inception_3a_pool = Conv2D(32, (1, 1), name='inception_3a_pool_conv')(inception_3a_pool)\n",
    "    inception_3a_pool = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_pool_bn')(inception_3a_pool)\n",
    "    inception_3a_pool = Activation('relu')(inception_3a_pool)\n",
    "    inception_3a_pool = ZeroPadding2D(padding=((3, 4), (3, 4)))(inception_3a_pool)\n",
    "\n",
    "    inception_3a_1x1 = Conv2D(64, (1, 1), name='inception_3a_1x1_conv')(x)\n",
    "    inception_3a_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3a_1x1_bn')(inception_3a_1x1)\n",
    "    inception_3a_1x1 = Activation('relu')(inception_3a_1x1)\n",
    "\n",
    "    inception_3a = concatenate([inception_3a_3x3, inception_3a_5x5, inception_3a_pool, inception_3a_1x1], axis=3)\n",
    "\n",
    "    # Inception3b\n",
    "    inception_3b_3x3 = Conv2D(96, (1, 1), name='inception_3b_3x3_conv1')(inception_3a)\n",
    "    inception_3b_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_3x3_bn1')(inception_3b_3x3)\n",
    "    inception_3b_3x3 = Activation('relu')(inception_3b_3x3)\n",
    "    inception_3b_3x3 = ZeroPadding2D(padding=(1, 1))(inception_3b_3x3)\n",
    "    inception_3b_3x3 = Conv2D(128, (3, 3), name='inception_3b_3x3_conv2')(inception_3b_3x3)\n",
    "    inception_3b_3x3 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_3x3_bn2')(inception_3b_3x3)\n",
    "    inception_3b_3x3 = Activation('relu')(inception_3b_3x3)\n",
    "\n",
    "    inception_3b_5x5 = Conv2D(32, (1, 1), name='inception_3b_5x5_conv1')(inception_3a)\n",
    "    inception_3b_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_5x5_bn1')(inception_3b_5x5)\n",
    "    inception_3b_5x5 = Activation('relu')(inception_3b_5x5)\n",
    "    inception_3b_5x5 = ZeroPadding2D(padding=(2, 2))(inception_3b_5x5)\n",
    "    inception_3b_5x5 = Conv2D(64, (5, 5), name='inception_3b_5x5_conv2')(inception_3b_5x5)\n",
    "    inception_3b_5x5 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_5x5_bn2')(inception_3b_5x5)\n",
    "    inception_3b_5x5 = Activation('relu')(inception_3b_5x5)\n",
    "\n",
    "    inception_3b_pool = Lambda(lambda x: x**2, name='power2_3b')(inception_3a)\n",
    "    inception_3b_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3))(inception_3b_pool)\n",
    "    inception_3b_pool = Lambda(lambda x: x*9, name='mult9_3b')(inception_3b_pool)\n",
    "    inception_3b_pool = Lambda(lambda x: K.sqrt(x), name='sqrt_3b')(inception_3b_pool)\n",
    "    inception_3b_pool = Conv2D(64, (1, 1), name='inception_3b_pool_conv')(inception_3b_pool)\n",
    "    inception_3b_pool = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_pool_bn')(inception_3b_pool)\n",
    "    inception_3b_pool = Activation('relu')(inception_3b_pool)\n",
    "    inception_3b_pool = ZeroPadding2D(padding=(4, 4))(inception_3b_pool)\n",
    "\n",
    "    inception_3b_1x1 = Conv2D(64, (1, 1), name='inception_3b_1x1_conv')(inception_3a)\n",
    "    inception_3b_1x1 = BatchNormalization(axis=3, epsilon=0.00001, name='inception_3b_1x1_bn')(inception_3b_1x1)\n",
    "    inception_3b_1x1 = Activation('relu')(inception_3b_1x1)\n",
    "\n",
    "    inception_3b = concatenate([inception_3b_3x3, inception_3b_5x5, inception_3b_pool, inception_3b_1x1], axis=3)\n",
    "\n",
    "    # Inception3c\n",
    "    inception_3c_3x3 = utils.conv2d_bn(inception_3b,\n",
    "                                       layer='inception_3c_3x3',\n",
    "                                       cv1_out=128,\n",
    "                                       cv1_filter=(1, 1),\n",
    "                                       cv2_out=256,\n",
    "                                       cv2_filter=(3, 3),\n",
    "                                       cv2_strides=(2, 2),\n",
    "                                       padding=(1, 1))\n",
    "\n",
    "    inception_3c_5x5 = utils.conv2d_bn(inception_3b,\n",
    "                                       layer='inception_3c_5x5',\n",
    "                                       cv1_out=32,\n",
    "                                       cv1_filter=(1, 1),\n",
    "                                       cv2_out=64,\n",
    "                                       cv2_filter=(5, 5),\n",
    "                                       cv2_strides=(2, 2),\n",
    "                                       padding=(2, 2))\n",
    "\n",
    "    inception_3c_pool = MaxPooling2D(pool_size=3, strides=2)(inception_3b)\n",
    "    inception_3c_pool = ZeroPadding2D(padding=((0, 1), (0, 1)))(inception_3c_pool)\n",
    "\n",
    "    inception_3c = concatenate([inception_3c_3x3, inception_3c_5x5, inception_3c_pool], axis=3)\n",
    "\n",
    "    #inception 4a\n",
    "    inception_4a_3x3 = utils.conv2d_bn(inception_3c,\n",
    "                                       layer='inception_4a_3x3',\n",
    "                                       cv1_out=96,\n",
    "                                       cv1_filter=(1, 1),\n",
    "                                       cv2_out=192,\n",
    "                                       cv2_filter=(3, 3),\n",
    "                                       cv2_strides=(1, 1),\n",
    "                                       padding=(1, 1))\n",
    "    inception_4a_5x5 = utils.conv2d_bn(inception_3c,\n",
    "                                       layer='inception_4a_5x5',\n",
    "                                       cv1_out=32,\n",
    "                                       cv1_filter=(1, 1),\n",
    "                                       cv2_out=64,\n",
    "                                       cv2_filter=(5, 5),\n",
    "                                       cv2_strides=(1, 1),\n",
    "                                       padding=(2, 2))\n",
    "\n",
    "    inception_4a_pool = Lambda(lambda x: x**2, name='power2_4a')(inception_3c)\n",
    "    inception_4a_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3))(inception_4a_pool)\n",
    "    inception_4a_pool = Lambda(lambda x: x*9, name='mult9_4a')(inception_4a_pool)\n",
    "    inception_4a_pool = Lambda(lambda x: K.sqrt(x), name='sqrt_4a')(inception_4a_pool)\n",
    "    inception_4a_pool = utils.conv2d_bn(inception_4a_pool,\n",
    "                                       layer='inception_4a_pool',\n",
    "                                       cv1_out=128,\n",
    "                                       cv1_filter=(1, 1),\n",
    "                                       padding=(2, 2))\n",
    "    inception_4a_1x1 = utils.conv2d_bn(inception_3c,\n",
    "                                       layer='inception_4a_1x1',\n",
    "                                       cv1_out=256,\n",
    "                                       cv1_filter=(1, 1))\n",
    "    inception_4a = concatenate([inception_4a_3x3, inception_4a_5x5, inception_4a_pool, inception_4a_1x1], axis=3)\n",
    "\n",
    "    #inception4e\n",
    "    inception_4e_3x3 = utils.conv2d_bn(inception_4a,\n",
    "                                       layer='inception_4e_3x3',\n",
    "                                       cv1_out=160,\n",
    "                                       cv1_filter=(1, 1),\n",
    "                                       cv2_out=256,\n",
    "                                       cv2_filter=(3, 3),\n",
    "                                       cv2_strides=(2, 2),\n",
    "                                       padding=(1, 1))\n",
    "    inception_4e_5x5 = utils.conv2d_bn(inception_4a,\n",
    "                                       layer='inception_4e_5x5',\n",
    "                                       cv1_out=64,\n",
    "                                       cv1_filter=(1, 1),\n",
    "                                       cv2_out=128,\n",
    "                                       cv2_filter=(5, 5),\n",
    "                                       cv2_strides=(2, 2),\n",
    "                                       padding=(2, 2))\n",
    "    inception_4e_pool = MaxPooling2D(pool_size=3, strides=2)(inception_4a)\n",
    "    inception_4e_pool = ZeroPadding2D(padding=((0, 1), (0, 1)))(inception_4e_pool)\n",
    "\n",
    "    inception_4e = concatenate([inception_4e_3x3, inception_4e_5x5, inception_4e_pool], axis=3)\n",
    "\n",
    "    #inception5a\n",
    "    inception_5a_3x3 = utils.conv2d_bn(inception_4e,\n",
    "                                       layer='inception_5a_3x3',\n",
    "                                       cv1_out=96,\n",
    "                                       cv1_filter=(1, 1),\n",
    "                                       cv2_out=384,\n",
    "                                       cv2_filter=(3, 3),\n",
    "                                       cv2_strides=(1, 1),\n",
    "                                       padding=(1, 1))\n",
    "\n",
    "    inception_5a_pool = Lambda(lambda x: x**2, name='power2_5a')(inception_4e)\n",
    "    inception_5a_pool = AveragePooling2D(pool_size=(3, 3), strides=(3, 3))(inception_5a_pool)\n",
    "    inception_5a_pool = Lambda(lambda x: x*9, name='mult9_5a')(inception_5a_pool)\n",
    "    inception_5a_pool = Lambda(lambda x: K.sqrt(x), name='sqrt_5a')(inception_5a_pool)\n",
    "    inception_5a_pool = utils.conv2d_bn(inception_5a_pool,\n",
    "                                       layer='inception_5a_pool',\n",
    "                                       cv1_out=96,\n",
    "                                       cv1_filter=(1, 1),\n",
    "                                       padding=(1, 1))\n",
    "    inception_5a_1x1 = utils.conv2d_bn(inception_4e,\n",
    "                                       layer='inception_5a_1x1',\n",
    "                                       cv1_out=256,\n",
    "                                       cv1_filter=(1, 1))\n",
    "\n",
    "    inception_5a = concatenate([inception_5a_3x3, inception_5a_pool, inception_5a_1x1], axis=3)\n",
    "\n",
    "    #inception_5b\n",
    "    inception_5b_3x3 = utils.conv2d_bn(inception_5a,\n",
    "                                       layer='inception_5b_3x3',\n",
    "                                       cv1_out=96,\n",
    "                                       cv1_filter=(1, 1),\n",
    "                                       cv2_out=384,\n",
    "                                       cv2_filter=(3, 3),\n",
    "                                       cv2_strides=(1, 1),\n",
    "                                       padding=(1, 1))\n",
    "    inception_5b_pool = MaxPooling2D(pool_size=3, strides=2)(inception_5a)\n",
    "    inception_5b_pool = utils.conv2d_bn(inception_5b_pool,\n",
    "                                       layer='inception_5b_pool',\n",
    "                                       cv1_out=96,\n",
    "                                       cv1_filter=(1, 1))\n",
    "    inception_5b_pool = ZeroPadding2D(padding=(1, 1))(inception_5b_pool)\n",
    "\n",
    "    inception_5b_1x1 = utils.conv2d_bn(inception_5a,\n",
    "                                       layer='inception_5b_1x1',\n",
    "                                       cv1_out=256,\n",
    "                                       cv1_filter=(1, 1))\n",
    "    inception_5b = concatenate([inception_5b_3x3, inception_5b_pool, inception_5b_1x1], axis=3)\n",
    "\n",
    "    av_pool = AveragePooling2D(pool_size=(3, 3), strides=(1, 1))(inception_5b)\n",
    "    reshape_layer = Flatten()(av_pool)\n",
    "    dense_layer = Dense(128, name='dense_layer')(reshape_layer)\n",
    "    norm_layer = Lambda(lambda  x: K.l2_normalize(x, axis=1), name='norm_layer')(dense_layer)\n",
    "\n",
    "\n",
    "    # Final Model\n",
    "    return Model(inputs=[myInput], outputs=norm_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "namelist=[]\n",
    "embeddinglist=[]\n",
    "\n",
    "K.clear_session()\n",
    "nn4_small2 = createmodel()\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "def select_triplets(embeddings, nrof_images_per_class, image_paths, people_per_batch, alpha):\n",
    "    \"\"\" Select the triplets for training\n",
    "    \"\"\"\n",
    "    trip_idx = 0\n",
    "    emb_start_idx = 0\n",
    "    num_trips = 0\n",
    "    triplets = []\n",
    "\n",
    "    # VGG Face: Choosing good triplets is crucial and should strike a balance between\n",
    "    #  selecting informative (i.e. challenging) examples and swamping training with examples that\n",
    "    #  are too hard. This is achieve by extending each pair (a, p) to a triplet (a, p, n) by sampling\n",
    "    #  the image n at random, but only between the ones that violate the triplet loss margin. The\n",
    "    #  latter is a form of hard-negative mining, but it is not as aggressive (and much cheaper) than\n",
    "    #  choosing the maximally violating example, as often done in structured output learning.\n",
    "\n",
    "    for i in range(people_per_batch):\n",
    "        nrof_images = int(nrof_images_per_class[i])\n",
    "        for j in range(1,nrof_images):\n",
    "            a_idx = emb_start_idx + j - 1\n",
    "            neg_dists_sqr = np.sum(np.square(embeddings[a_idx] - embeddings), 1)\n",
    "            for pair in range(j, nrof_images): # For every possible positive pair.\n",
    "                p_idx = emb_start_idx + pair\n",
    "                pos_dist_sqr = np.sum(np.square(embeddings[a_idx]-embeddings[p_idx]))\n",
    "                neg_dists_sqr[emb_start_idx:emb_start_idx+nrof_images] = np.NaN\n",
    "                all_neg = np.where(np.logical_and(neg_dists_sqr-pos_dist_sqr<alpha, pos_dist_sqr<neg_dists_sqr))[0]  # FaceNet selection\n",
    "                # all_neg = np.where(neg_dists_sqr-pos_dist_sqr<alpha)[0] # VGG Face selecction\n",
    "                nrof_random_negs = all_neg.shape[0]\n",
    "                if nrof_random_negs>0:\n",
    "                    rnd_idx = np.random.randint(nrof_random_negs)\n",
    "                    n_idx = all_neg[rnd_idx]\n",
    "                    triplets.append((image_paths[a_idx], image_paths[p_idx], image_paths[n_idx]))\n",
    "                    #print('Triplet %d: (%d, %d, %d), pos_dist=%2.6f, neg_dist=%2.6f (%d, %d, %d, %d, %d)' %\n",
    "                    #    (trip_idx, a_idx, p_idx, n_idx, pos_dist_sqr, neg_dists_sqr[n_idx], nrof_random_negs, rnd_idx, i, j, emb_start_idx))\n",
    "                    trip_idx += 1\n",
    "\n",
    "                num_trips += 1\n",
    "\n",
    "        emb_start_idx += nrof_images\n",
    "\n",
    "    np.random.shuffle(triplets)\n",
    "    return triplets, num_trips, len(triplets)\n",
    "\n",
    "def sample_people(dataset, people_per_batch, images_per_person):\n",
    "    nrof_images = people_per_batch * images_per_person\n",
    "\n",
    "    # Sample classes from the dataset\n",
    "    nrof_classes = len(dataset)\n",
    "    class_indices = np.arange(nrof_classes)\n",
    "    np.random.shuffle(class_indices)\n",
    "    i = 0\n",
    "    image_paths = []\n",
    "    num_per_class = []\n",
    "    sampled_class_indices = []\n",
    "    # Sample images from these classes until we have enough\n",
    "    while len(image_paths)<nrof_images:\n",
    "        class_index = class_indices[i]\n",
    "        nrof_images_in_class = len(dataset[class_index])\n",
    "        image_indices = np.arange(nrof_images_in_class)\n",
    "        np.random.shuffle(image_indices)\n",
    "        nrof_images_from_class = min(nrof_images_in_class, images_per_person, nrof_images-len(image_paths))\n",
    "        idx = image_indices[0:nrof_images_from_class]\n",
    "        image_paths_for_class = [dataset[class_index].image_paths[j] for j in idx]\n",
    "        sampled_class_indices += [class_index]*nrof_images_from_class\n",
    "        image_paths += image_paths_for_class\n",
    "        num_per_class.append(nrof_images_from_class)\n",
    "        i+=1\n",
    "\n",
    "    return image_paths, num_per_class\n",
    "\n",
    "class ImageClass():\n",
    "    \"Stores the paths to images for a given class\"\n",
    "    def __init__(self, name, image_paths):\n",
    "        self.name = name\n",
    "        self.image_paths = image_paths\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name + ', ' + str(len(self.image_paths)) + ' images'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "def get_dataset(path, has_class_directories=True):\n",
    "    dataset = []\n",
    "    path_exp = os.path.expanduser(path)\n",
    "    print(path_exp)\n",
    "    classes = [path for path in os.listdir(path_exp) \\\n",
    "                    if os.path.isdir(os.path.join(path_exp, path))]\n",
    "#     print(classes)\n",
    "    classes.sort()\n",
    "    nrof_classes = len(classes)\n",
    "    for i in range(nrof_classes):\n",
    "        class_name = classes[i]\n",
    "        facedir = os.path.join(path_exp, class_name)\n",
    "        image_paths = get_image_paths(facedir)\n",
    "        dataset.append(ImageClass(class_name, image_paths))\n",
    "    return dataset\n",
    "\n",
    "def triplet_path_to_numpy_representation(triplet_array):\n",
    "    triplet_numpy_array = []\n",
    "    for triplet in triplet_array:\n",
    "        # print(triplet)\n",
    "        triplet_numpy = []\n",
    "        for individual_image in triplet:\n",
    "            img=cv2.imread(individual_image)\n",
    "            img=cv2.resize(img,(96,96))\n",
    "            triplet_numpy.append(img)\n",
    "        triplet_numpy_array.append(triplet_numpy)\n",
    "    return triplet_numpy_array\n",
    "\n",
    "def get_image_paths(facedir):\n",
    "    image_paths = []\n",
    "    if os.path.isdir(facedir):\n",
    "        images = os.listdir(facedir)\n",
    "        image_paths = [os.path.join(facedir,img) for img in images]\n",
    "    return image_paths\n",
    "\n",
    "def triplet_generator(people_per_batch, images_per_person,embedding_size,alpha):\n",
    "    ''' Dummy triplet generator for API usage demo only.\n",
    "\n",
    "    Will be replaced by a version that uses real image data later.\n",
    "\n",
    "    :return: a batch of (anchor, positive, negative) triplets\n",
    "    '''\n",
    "    import tensorflow as tf\n",
    "    global graph\n",
    "    with graph.as_default():\n",
    "        dataset=get_dataset(\"test\")\n",
    "        nrof_examples = people_per_batch * images_per_person\n",
    "        while True:\n",
    "#             print(tf.get_default_graph)\n",
    "            image_paths, num_per_class = sample_people(dataset, people_per_batch, images_per_person)\n",
    "            labels_array = np.arange(nrof_examples)\n",
    "            image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\n",
    "            # sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\n",
    "            emb_array = np.zeros((nrof_examples, embedding_size))\n",
    "            # nrof_batches = int(np.ceil(nrof_examples / batch_size))\n",
    "            # for i in range(nrof_batches):\n",
    "            # batch_size = min(nrof_examples-i*batch_size, batch_size)\n",
    "#             print(\"These are image paths\")\n",
    "#             print()\n",
    "#             print(image_paths)\n",
    "            triplet_numpy_array=triplet_path_to_numpy_representation(image_paths_array)\n",
    "#             namelist.append(image_paths_array)\n",
    "#             embeddinglist.append(triplet_numpy_array)\n",
    "            triplet_numpy_array=np.array(triplet_numpy_array)\n",
    "#             print(\"This is sample triplet numpy array shape\")\n",
    "#             print()\n",
    "#             print(triplet_numpy_array.shape)\n",
    "            \n",
    "#             print(\"This is sample triplet numpy array \")\n",
    "#             print()\n",
    "#             print(triplet_numpy_array[0][0][0])\n",
    "            \n",
    "            emb=nn4_small2.predict(np.reshape(triplet_numpy_array,(-1,96,96,3)))\n",
    "#             print(\"This is embedding\")\n",
    "#             print()\n",
    "#             print(emb)\n",
    "    #         xlist.append(emb)\n",
    "            # emb=model.predict(np.reshape(np.array(triplet_numpy_array),(-1,200,180,3)))\n",
    "            emb_array[labels_array,:] = emb\n",
    "            triplets, nrof_random_negs, nrof_triplets = select_triplets(emb_array, num_per_class,\n",
    "                image_paths, people_per_batch, alpha)\n",
    "#             print(\"These are triplets\")\n",
    "#             print()\n",
    "#             print(triplets)\n",
    "            labels_array = np.reshape(np.arange(len(triplets)*3),(-1,3))\n",
    "            triplet_paths_array = np.reshape(np.expand_dims(np.array(triplets),1), (-1,3))\n",
    "            triplet_paths_array=np.array(triplet_path_to_numpy_representation(triplet_paths_array))\n",
    "#             print(\"This triplet_paths_array.shape\")\n",
    "#             print()\n",
    "#             print(triplet_paths_array.shape)\n",
    "            reshaped_triplet=np.reshape(triplet_paths_array,(-1,96,96,3))\n",
    "#             print(\"This reshaped_triplet.shape\")\n",
    "#             print()\n",
    "#             print(reshaped_triplet.shape)\n",
    "            predictions=nn4_small2.predict(reshaped_triplet)\n",
    "#             print(\"This predictions.shape\")\n",
    "#             print()\n",
    "#             print(predictions.shape)\n",
    "            \n",
    "#             print(\"This is predictions\")\n",
    "#             print()\n",
    "#             print(predictions)\n",
    "#             temp=np.hsplit(triplet_paths_array,3)\n",
    "#             a_batch = np.reshape(temp[0],(-1,96,96,3))\n",
    "#             p_batch = np.reshape(temp[1],(-1,96,96,3))\n",
    "#             n_batch = np.reshape(temp[2],(-1,96,96,3))\n",
    "#             a_predict=nn4_small2.predict(a_batch)\n",
    "#             p_predict=nn4_small2.predict(p_batch)\n",
    "#             n_predict=nn4_small2.predict(n_batch)\n",
    "            yield reshaped_triplet,predictions\n",
    "        # a_batch = np.reshape(temp[0],(-1,200,180,3))\n",
    "        # p_batch = np.reshape(temp[1],(-1,200,180,3))\n",
    "        # n_batch = np.reshape(temp[2],(-1,200,180,3))\n",
    "#         yield [a_batch , p_batch, n_batch], None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " def triplet_loss_v2(y_true,y_preds):\n",
    "    print()\n",
    "    print(\"Yo,this is the loss fn,thanks for calling\")\n",
    "    print()\n",
    "    reshape_triplets_embeddings=tf.reshape(y_preds,[-1,3,128])\n",
    "    an,pn,nn=tf.split(reshape_triplets_embeddings,3,1)\n",
    "    a=tf.reshape(an,[-1,128])\n",
    "    p=tf.reshape(pn,[-1,128])\n",
    "    n=tf.reshape(nn,[-1,128])\n",
    "    p_dist = K.sum(K.square(a-p), axis=-1)\n",
    "    n_dist = K.sum(K.square(a-n), axis=-1)\n",
    "    return K.sum(K.maximum(p_dist - n_dist, 0), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Yo,this is the loss fn,thanks for calling\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "rms=optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=1e-4)\n",
    "nn4_small2.compile(loss=triplet_loss_v2, optimizer=rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\amark\\anaconda3\\envs\\python3.5\\lib\\site-packages\\ipykernel_launcher.py:35: RuntimeWarning: invalid value encountered in less\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/5 [===========>..................] - ETA: 58s - loss: nan    "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (0,) for Tensor 'norm_layer_target:0', which has shape '(?, ?)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-7914d33641f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtriplet_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnn4_small2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\amark\\anaconda3\\envs\\python3.5\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\amark\\anaconda3\\envs\\python3.5\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2175\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2176\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2177\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2179\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\amark\\anaconda3\\envs\\python3.5\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1847\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1848\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1849\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1851\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\amark\\anaconda3\\envs\\python3.5\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2475\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2476\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\amark\\anaconda3\\envs\\python3.5\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\amark\\anaconda3\\envs\\python3.5\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m                 \u001b[1;34m'which has shape %r'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1104\u001b[1;33m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m   1105\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (0,) for Tensor 'norm_layer_target:0', which has shape '(?, ?)'"
     ]
    }
   ],
   "source": [
    "generator = triplet_generator(5,3,128,0.2) \n",
    "nn4_small2.fit_generator(generator, epochs=5, steps_per_epoch=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
